<!DOCTYPE html><html><head><meta charset="utf-8"><style>body {
  max-width: 980px;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headeranchor-link {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .headeranchor-link:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .headeranchor,
.markdown-body h2 .headeranchor,
.markdown-body h3 .headeranchor,
.markdown-body h4 .headeranchor,
.markdown-body h5 .headeranchor,
.markdown-body h6 .headeranchor {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .headeranchor-link,
.markdown-body h2:hover .headeranchor-link,
.markdown-body h3:hover .headeranchor-link,
.markdown-body h4:hover .headeranchor-link,
.markdown-body h5:hover .headeranchor-link,
.markdown-body h6:hover .headeranchor-link {
  height: 1em;
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .headeranchor-link .headeranchor,
.markdown-body h2:hover .headeranchor-link .headeranchor,
.markdown-body h3:hover .headeranchor-link .headeranchor,
.markdown-body h4:hover .headeranchor-link .headeranchor,
.markdown-body h5:hover .headeranchor-link .headeranchor,
.markdown-body h6:hover .headeranchor-link .headeranchor {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* Multimarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\f05c';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><title>report_draft</title></head><body><article class="markdown-body"><h1 id="abstract"><a name="user-content-abstract" href="#abstract" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Abstract</h1>
<p>Libraries often have large archives of legacy video recordings that lack reliable segmentation and metadata for efficient research, known as the &ldquo;digital silo&rdquo; problem. We propose a state-of-the-art multimodal method that combines visual, audio and text cues to automatically segment TV news videos by their natural program boundaries and generate metadata such as topics. </p>
<h1 id="introduction"><a name="user-content-introduction" href="#introduction" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Introduction</h1>
<p>While there has been extensive research on story-level segmentation of TV news programs, to our knowledsge, program-level segmentation has not been studied so much yet.  </p>
<p>Intuitively story-level boundaries must be easier to classify as story, by definition, is a logical group of frames based on specific topics. TV news programs may not necessarily be. The task of dividing TV news recordings by their program boundaries can be challenging even for humans. For instance, CNN had relatively undifferentiated programming back-to-back for hours at a time in the 90s and it can be difficult to locate program boundaries. That said, it may be safe to assume program boundaries are a subset of story boundaries, making story boundaries as decision units for final binary classification for program boundaries.</p>
<p>It may be possible to directly classify program boundaries without finding story boundaries. This avenue is to be explored as the project evolves. Another reason using story boundaries as a program boundary candidate may be a good idea is because the existing caption files have marks that indicate the start of stories and commericals (e.g. SEG_0 type=Story starts). Such marks can be thought of as free labelled samples for training a story boundary classifier.</p>
<p>TBC&hellip;</p>
<h1 id="features"><a name="user-content-features" href="#features" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Features</h1>
<p>In this section, we list features that we believe to have some discriminative power for or correlations with program/story boundaries. Note this is just a tentative laundry list of features to be explored in more detail. Some of the useless features will be pruned in the future. Also notice none of the features will be strong enough to classify program boundaries as a standalone feature. Therefore the key to success would be to develop learning algorithms to fuse the multimodal features properly.</p>
<p>At a high level, some of the features will be used for story segmentation while others may be directly used for program segmentation. There will be three separate feature extration streams: video, audio, and text. The streams flow independently of each other and only at a final stage will they be combined.</p>
<h2 id="video-features"><a name="user-content-video-features" href="#video-features" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Video Features</h2>
<h3 id="anchor-shot-detection"><a name="user-content-anchor-shot-detection" href="#anchor-shot-detection" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Anchor Shot Detection</h3>
<p>We take advantage of the fact that the scenes that include anchors (or their faces) are correlated with story transitions. We sample a frame at regular or noisy intervals and detect human faces. We then run a K-mean clustering method on the detected faces. The dominant cluster centroid and faces assigned to the class of the cluster most likely will be anchor faces. We assume anchor faces most likely appear most often, given a long time horizon (say 1 hour).</p>
<p>Also another visual cue that hints at story transition is, given an anchor shot, to check whether the location of the anchor face (bounding box of the face) is tilted to the left or right, deviating from the center of the frame. When story transitions take place in news programs, it is common that an anchor face is placed to the left with an image of the next story over the anchor&rsquo;s shoulder. </p>
<p>TBC&hellip;</p>
<p>Most shows have consistent anchors or presenters in unique sets (again, with some possible re-use within the same channel and no re-use across channels). They might appear as guests on other shows or special coverage, but their appearance on their own show should be consistent in format and repetition.</p>
<h3 id="black-junk-frames"><a name="user-content-black-junk-frames" href="#black-junk-frames" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Black Junk Frames</h3>
<p>It has been reported in some literature that commercials or news stories are followed by one or two consecutive frames that are completely black/blank. Such black frames do appear in the middle of stories or in other places.</p>
<h3 id="shot-boundaries"><a name="user-content-shot-boundaries" href="#shot-boundaries" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Shot Boundaries</h3>
<p>In TRECVID dataset, 94+% of the story boundaries appear in the vicinity of a shot boundary. The same would likely go for program boundaries. Rather than computing this as a binary feature, we can take this as a continuous real-valued score to be used for story boundary detection. </p>
<h3 id="logotitle-detection"><a name="user-content-logotitle-detection" href="#logotitle-detection" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Logo/Title Detection</h3>
<p>Most shows have a distinctive set of logo and title visuals. Logo or title candidates can be detected by locating a region whose pixels do not change over many frames. For more details, <a href="https://www.hindawi.com/journals/ijdmb/2012/732514/">https://www.hindawi.com/journals/ijdmb/2012/732514/</a></p>
<h3 id="visual-motion-acitivity"><a name="user-content-visual-motion-acitivity" href="#visual-motion-acitivity" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Visual Motion Acitivity</h3>
<p>We can measure how much video is changing with a color pixel difference tracking method, using the percentage of pixels that have changed color between it and the previous frame. </p>
<p>image saliency or tranformed frames in Fourier domain may be useful&hellip;</p>
<h3 id="credit-information"><a name="user-content-credit-information" href="#credit-information" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Credit Information</h3>
<p>closing credit info screen.<br />
copyright/closing credit info</p>
<h2 id="audio-features"><a name="user-content-audio-features" href="#audio-features" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Audio Features</h2>
<p>audio (mp3) can be extracted from original mp4 files using avconv. feeding video into an audio-related feature extractor would be computationally expensive.  </p>
<h3 id="speaker-detection"><a name="user-content-speaker-detection" href="#speaker-detection" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Speaker Detection</h3>
<p>Changes of a speaker may be correlated with story/program transition. There are a few tools to detect the speaker change, notably based on normalized cross likelihood ratio (NCLR).</p>
<h3 id="silence-detection"><a name="user-content-silence-detection" href="#silence-detection" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Silence Detection</h3>
<p>It has been shown long audio pauses are strong indicators of story transitions. This audio feature capitalizes on the pattern that an achorperson pauses before moving on to introduce a new story. One way to estimate the pause duration is to track a maximum time period where volume at timestep $t$ goes below some reference threshold (e.g. fixed/rolling average volume of a given newstream). Such maximum low-volume period is calculated within a regular sampling interval and defined to estimate a pause period.</p>
<p>The long pause period together with some common transition/closing/opening lines may be very useful. Then again, this feature may not be so useful, as opening and closing transitions usually coincide with theme music.</p>
<h3 id="transition-music"><a name="user-content-transition-music" href="#transition-music" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Transition Music</h3>
<p>TBC&hellip;</p>
<p>Distinctive theme songs (might be the same across broadcasts within a channel [i.e. same music might be used for KABC’s 5 p.m. and 6 p.m. newscasts, but that theme will be  different than KNBC uses for their newscasts]). There should be high stability in these theme songs within a given season and network, but possible variation across them.</p>
<p>Using this feature &hellip; </p>
<h2 id="text-features"><a name="user-content-text-features" href="#text-features" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Text Features</h2>
<p>While a large proportion of the video data come with the caption files, there are still many video files that do not have one. Some have one but the caption file is just incomplete. For the caption-lacking videso, we can consider two approaches:</p>
<p><em>Automatic Speech Recognition on audio files<br />
</em>Video captioning through Densecap</p>
<p>ASR may be the way to go for the input compatibility. Both will generate text.</p>
<h3 id="bag-of-words-histogram-distance"><a name="user-content-bag-of-words-histogram-distance" href="#bag-of-words-histogram-distance" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Bag Of Words Histogram Distance</h3>
<p>Words used for one story may be difficult from those used in another story. We build a vocabulary/dictionary vector out of commonly used words along with the existing caption samples and tally the word occurences within a regular time interval. The result will be a Bag-of-Words histogram. We then calculate the chi-square distance between the two consecutive BoW histograms. If the distance will be real-valued, continuous. </p>
<h3 id="topical-distance"><a name="user-content-topical-distance" href="#topical-distance" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Topical Distance</h3>
<p>We utilize the fact that story is a topical group of frames. Two popular methods used for topic modelling are non-negative matrix factorization (NMF) and Latent Dirichlet Allocation (LDA). We can divide a caption (8hour-long in video) into small documents (say 5minutes or some sensible average duration of stories), and run a trained LDA model on the documents which will produce outputs simliar to: </p>
<p>doc1: topic A 50%, topic B 30% &hellip;</p>
<p>we can then calculate a L1/L2 distance between the topic mixtures of each documents. If there&rsquo;s a significant difference, it could strongly indiciate a story transition or even news program transition. (commericals will likely have distinct topic mixtures)</p>
<p>One challenge is to choose the number of topics in advance, which is a hyper parameter. </p>
<p>It seems LDA is going out of fashion and may consider other state-of-the-art methods instead. <a href="https://datascience.stackexchange.com/questions/678/what-are-some-standard-ways-of-computing-the-distance-between-documents">https://datascience.stackexchange.com/questions/678/what-are-some-standard-ways-of-computing-the-distance-between-documents</a></p>
<h3 id="transition-markers"><a name="user-content-transition-markers" href="#transition-markers" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Transition Markers</h3>
<p>There seem to be a few keywords that appear recurringly in the caption files and they seem to be correlated with program boundaries.</p>
<p>Such keywords (case insensitive) are: caption, commercial, story, SEG.<br />
<em>caption: caption by, captioning by&hellip;<br />
</em>commerical: type=commercial. marks the start of commercial<br />
<em>story: marks the start of story segment<br />
</em>SEG: marks a story transition.</p>
<p>However, this applies only to videos with the captions. Moreover, some caption files are incomplete (like in 1972-01-07) or don&rsquo;t have the keywords where they should be (=a new story starts and there&rsquo;s no SEG type=story.) This skippy behavior applies to all the keywords. Keywords alone are incomplete.<br />
e.g. 2006-06-13_0000_US_00000141_V11_MB12_VHS13_H2_JK.txt3</p>
<p>cc-keyword-script is used..</p>
<h3 id="transition-phrases"><a name="user-content-transition-phrases" href="#transition-phrases" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Transition Phrases</h3>
<p>There are repeating lines highly correlated with the story/program boundaries such as greetings. We can manually build a collection of reliable transition phrases or find one from the web.</p>
<h3 id="vcr-index"><a name="user-content-vcr-index" href="#vcr-index" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>VCR Index</h3>
<p>This is a hacky feature that may be useful for the given dataset, but won&rsquo;t generalize over other news program datasets. The indices of VCRs used for recording are specified in the filenames. The indices may give hint for channel names and recurring structure of programs.</p>
<p>For example, V2 seems to record NBC channels on a regular recording schedule. V2_2006_03_01 may have a simliar structure as the next day V2_2006_03_02 (for daily programs) and a week later V2_2006_03_08 (for weekly programs) </p>
<p><em>v0: <br />
</em>v1: WCBS-TV (cbs)<br />
<em>v2: NBC<br />
</em>v3: ABC<br />
<em>v4: <br />
</em>v5: <br />
<em>v6: <br />
</em>v7: <br />
<em>v8: <br />
</em>v9:<br />
*v10:</p>
<p>+The VCR index to TV channel mapping seems preserved throughout the years. Otherwise, using this feature may not be a good idea.</p>
<h1 id="multimodal-fusion"><a name="user-content-multimodal-fusion" href="#multimodal-fusion" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Multimodal Fusion</h1>
<p>&hellip;</p>
<h1 id="system-overview"><a name="user-content-system-overview" href="#system-overview" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>System Overview</h1>
<p>input preprocessing using pretrained image classifier (producing visual feature map&hellip;)</p>
<h1 id="experiments"><a name="user-content-experiments" href="#experiments" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Experiments</h1>
<h2 id="inputs"><a name="user-content-inputs" href="#inputs" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Inputs</h2>
<p>We currently have:<br />
<em>video (mp4)<br />
</em>audio: none. can easily be extracted into mp3 or other audio formats.<br />
*text: caption files (srt,txt3,tx4) for videos recorded roughly since 2000. we can use ASR or Video captioning tools to generate captions for the videos that do not have caption files yet.</p>
<h2 id="evaluation-metrics"><a name="user-content-evaluation-metrics" href="#evaluation-metrics" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Evaluation Metrics</h2>
<h2 id="solution-model"><a name="user-content-solution-model" href="#solution-model" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Solution Model</h2>
<h1 id="conclusion"><a name="user-content-conclusion" href="#conclusion" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Conclusion</h1>
<h2 id="challenges"><a name="user-content-challenges" href="#challenges" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Challenges</h2>
<p>That said, some networks have less distinctive show boundaries. CNN, for example, had relatively undifferentiated programming for hours at a time in the 90s. In more recent years they have a clearer signal of different shows. </p>
<h2 id="further-work"><a name="user-content-further-work" href="#further-work" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Further Work</h2>
<ul>
<li>shot boundaries may not be useful (just like phonetics was not useful in speech recognition)=</li>
<li>smallest decision unit: segment based on shot boundaries or histogram difference between two frames, entropy, video saliency</li>
<li>medium decision unit: segment based on stories and learn to combine them to a program&hellip; anchor shot&hellip;</li>
<li>largest decision: unit program</li>
</ul>
<p>three separate input streams<br />
<em> audio-&gt; feature map<br />
</em> text-&gt; feature map<br />
* video-&gt; feature map</p>
<h1 id="related-work"><a name="user-content-related-work" href="#related-work" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Related Work</h1>
<p>need to filter out by priority<br />
need to categorize</p>
<p><a href="https://pdfs.semanticscholar.org/41ed/c1f04cef2af8aa112642a0d3fdc36a395dda.pdf">https://pdfs.semanticscholar.org/41ed/c1f04cef2af8aa112642a0d3fdc36a395dda.pdf</a><br />
the appearance of an anchor person, audio pitch jump and significant audio pauses</p>
<p><a href="http://crcv.ucf.edu/papers/civr2005_zhai.pdf">http://crcv.ucf.edu/papers/civr2005_zhai.pdf</a><br />
obtained text from Automatic Speech Recognition (ASR) to detect potential segmentation points</p>
<p><a href="http://www.ee.columbia.edu/ln/dvmm/publications/03/icme2003pr.pdf">http://www.ee.columbia.edu/ln/dvmm/publications/03/icme2003pr.pdf</a><br />
Hsu et al. used a maximum entropy objective to select the most informative mid-level audio and video features and demonstrated an optimal feature fusion method. </p>
<p><a href="http://www.ee.columbia.edu/ln/dvmm/publications/04/hsu04generative.pdf">http://www.ee.columbia.edu/ln/dvmm/publications/04/hsu04generative.pdf</a><br />
<a href="http://www.ee.columbia.edu/~lyndon/pubs/spie2004-seg.pdf">http://www.ee.columbia.edu/~lyndon/pubs/spie2004-seg.pdf</a><br />
In later works, Hsu et al.  investigated alternative discriminative models, i.e. Support Vector Machine (SVM), and showed a performance improvement when combining maximum entropy with SVM.</p>
<p>link:<br />
While Jianping et al.presented a Naive Bayes approach for story boundary detection,</p>
<p><a href="http://mmlab.ie.cuhk.edu.hk/archive/2002/CSVT02_Video.pdf">http://mmlab.ie.cuhk.edu.hk/archive/2002/CSVT02_Video.pdf</a><br />
Gao et al. [7] combined syntactic and semantic methods for segmentation using an unsupervised learning method.</p>
<p><a href="http://lxie.nwpu-aslp.org/papers/2012-IEICE-WangXX-A2-SCI-EI-JNL.pdf">http://lxie.nwpu-aslp.org/papers/2012-IEICE-WangXX-A2-SCI-EI-JNL.pdf</a><br />
description: Broadcast News Story Segmentation Using Conditional Random Field</p>
<p><a href="https://pdfs.semanticscholar.org/5c21/6db7892fa3f515d816f84893bfab1137f0b2.pdf">https://pdfs.semanticscholar.org/5c21/6db7892fa3f515d816f84893bfab1137f0b2.pdf</a><br />
existence of blank frames </p>
<p><a href="http://www.cs.cmu.edu/~mychen/publication/duygulu_ICME04.pdf">http://www.cs.cmu.edu/~mychen/publication/duygulu_ICME04.pdf</a><br />
existence of blank frames</p>
<ul>
<li><a href="https://books.google.co.kr/books?id=nCnSy5XXdygC&amp;pg=PA361&amp;lpg=PA361&amp;dq=boundary+segmentation+detection+news+program&amp;source=bl&amp;ots=mxpZjNw_d6&amp;sig=JsdarYROgzMCxP_ssi8vlwqRrSA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjkzKe4yKvUAhWBkJQKHaizD_cQ6AEIYTAJ#v=onepage&amp;q=boundary%20segmentation%20detection%20news%20program&amp;f=false">https://books.google.co.kr/books?id=nCnSy5XXdygC&amp;pg=PA361&amp;lpg=PA361&amp;dq=boundary+segmentation+detection+news+program&amp;source=bl&amp;ots=mxpZjNw_d6&amp;sig=JsdarYROgzMCxP_ssi8vlwqRrSA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjkzKe4yKvUAhWBkJQKHaizD_cQ6AEIYTAJ#v=onepage&amp;q=boundary%20segmentation%20detection%20news%20program&amp;f=false</a></li>
<li><a href="http://www.inesc-id.pt/pt/indicadores/Ficheiros/1146.pdf">http://www.inesc-id.pt/pt/indicadores/Ficheiros/1146.pdf</a></li>
<li><a href="https://books.google.co.kr/books?id=50hnLI_Jz3cC&amp;pg=PA4&amp;lpg=PA4&amp;dq=boundary+segmentation+detection+news+program&amp;source=bl&amp;ots=ngB8rCxVd4&amp;sig=_ofo53vnQtFC3oCzYhUepQAnEEo&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiRuM7hx6vUAhUHTbwKHXaMD58Q6AEIVjAH#v=onepage&amp;q=boundary%20segmentation%20detection%20news%20program&amp;f=false">https://books.google.co.kr/books?id=50hnLI_Jz3cC&amp;pg=PA4&amp;lpg=PA4&amp;dq=boundary+segmentation+detection+news+program&amp;source=bl&amp;ots=ngB8rCxVd4&amp;sig=_ofo53vnQtFC3oCzYhUepQAnEEo&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiRuM7hx6vUAhUHTbwKHXaMD58Q6AEIVjAH#v=onepage&amp;q=boundary%20segmentation%20detection%20news%20program&amp;f=false</a></li>
<li><a href="http://www.cs.cmu.edu/~mehrbod/SSeg07.pdf">http://www.cs.cmu.edu/~mehrbod/SSeg07.pdf</a></li>
<li><a href="http://www1.cs.columbia.edu/~smaskey/candidacy/cand_papers/merlino_navigation_story_seg.pdf">http://www1.cs.columbia.edu/~smaskey/candidacy/cand_papers/merlino_navigation_story_seg.pdf</a></li>
<li><a href="https://pdfs.semanticscholar.org/5c21/6db7892fa3f515d816f84893bfab1137f0b2.pdf">https://pdfs.semanticscholar.org/5c21/6db7892fa3f515d816f84893bfab1137f0b2.pdf</a></li>
<li><a href="http://cs229.stanford.edu/proj2012/DaneshiYu-BroadcastNews%20StoryBoundaryDetectionUsingVisual,AudioAndTextFeatures.pdf">http://cs229.stanford.edu/proj2012/DaneshiYu-BroadcastNews%20StoryBoundaryDetectionUsingVisual,AudioAndTextFeatures.pdf</a></li>
<li><a href="https://www.hindawi.com/journals/ijdmb/2012/732514/">https://www.hindawi.com/journals/ijdmb/2012/732514/</a></li>
<li><a href="http://medialab.sjtu.edu.cn/publications/2015/2015_BMSB_Wenjing.pdf">http://medialab.sjtu.edu.cn/publications/2015/2015_BMSB_Wenjing.pdf</a></li>
<li><a href="https://github.com/MaxReimann/Shot-Boundary-Detection/blob/master/paper/SBD-Approach-Paper.pdf">https://github.com/MaxReimann/Shot-Boundary-Detection/blob/master/paper/SBD-Approach-Paper.pdf</a></li>
<li><a href="https://sites.google.com/site/distributedlittleredhen/summer-of-code/rsoc15report#Vasanth">https://sites.google.com/site/distributedlittleredhen/summer-of-code/rsoc15report#Vasanth</a></li>
<li><a href="https://sites.google.com/site/distributedlittleredhen/summer-of-code/rsoc15report#Mattia">https://sites.google.com/site/distributedlittleredhen/summer-of-code/rsoc15report#Mattia</a></li>
<li><a href="https://arxiv.org/pdf/1411.4389.pdf">https://arxiv.org/pdf/1411.4389.pdf</a></li>
<li><a href="https://github.com/yosinski/deep-visualization-toolbox">https://github.com/yosinski/deep-visualization-toolbox</a></li>
<li><a href="https://www.slideshare.net/RJIonline/newsscape-preserving-tv-news">https://www.slideshare.net/RJIonline/newsscape-preserving-tv-news</a></li>
<li><a href="http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202009/pdfs/0001957.pdf">http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202009/pdfs/0001957.pdf</a></li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/archive/2002/CSVT02_Video.pdf">http://mmlab.ie.cuhk.edu.hk/archive/2002/CSVT02_Video.pdf</a></li>
<li><a href="http://www.quaero.org/media/files/bibliographie/bredin_segmentation_of_tv_icassp2012.pdf">http://www.quaero.org/media/files/bibliographie/bredin_segmentation_of_tv_icassp2012.pdf</a></li>
<li><a href="http://www.cstr.ed.ac.uk/downloads/publications/2000/asr2000.pdf">http://www.cstr.ed.ac.uk/downloads/publications/2000/asr2000.pdf</a></li>
<li><a href="http://www.bcs.org/upload/pdf/ewic_im99_paper3.pdf">http://www.bcs.org/upload/pdf/ewic_im99_paper3.pdf</a></li>
<li><a href="https://www.hindawi.com/journals/ijdmb/2012/732514/">https://www.hindawi.com/journals/ijdmb/2012/732514/</a></li>
<li><a href="https://pdfs.semanticscholar.org/8516/3a2b8998920cb1655ec81550092fafbf3888.pdf">https://pdfs.semanticscholar.org/8516/3a2b8998920cb1655ec81550092fafbf3888.pdf</a></li>
<li><a href="http://www.ee.columbia.edu/~lyndon/pubs/adventtr2005-seg.pdf">http://www.ee.columbia.edu/~lyndon/pubs/adventtr2005-seg.pdf</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=2106118">http://dl.acm.org/citation.cfm?id=2106118</a></li>
<li><a href="http://www.cai.sk/ojs/index.php/cai/article/viewFile/185/156">http://www.cai.sk/ojs/index.php/cai/article/viewFile/185/156</a></li>
<li><a href="http://class.inrialpes.fr/pub/poulisse-ihci09.pdf">http://class.inrialpes.fr/pub/poulisse-ihci09.pdf</a></li>
<li><a href="http://mklab.iti.gr/project/video-shot-segm">http://mklab.iti.gr/project/video-shot-segm</a></li>
<li><a href="http://johmathe.name/shotdetect.html">http://johmathe.name/shotdetect.html</a></li>
<li><a href="https://github.com/yahoo/hecate">https://github.com/yahoo/hecate</a><br />
<em><a href="https://github.com/andrefaraujo/videosearch"><a href="https://github.com/andrefaraujo/videosearch">https://github.com/andrefaraujo/videosearch</a></a>  <br />
</em><a href="https://github.com/Breakthrough/PySceneDetect">https://github.com/Breakthrough/PySceneDetect</a><br />
<em><a href="https://github.com/yuhonglin/shotdetect"><a href="https://github.com/yuhonglin/shotdetect">https://github.com/yuhonglin/shotdetect</a></a><br />
</em><a href="https://github.com/gberta/HFL_code">https://github.com/gberta/HFL_code</a><br />
*<a href="https://github.com/MaxReimann/Shot-Boundary-Detection">https://github.com/MaxReimann/Shot-Boundary-Detection</a></li>
<li><a href="https://arxiv.org/abs/1705.08214">https://arxiv.org/abs/1705.08214</a><br />
<em><a href="https://arxiv.org/pdf/1610.00211.pdf"><a href="https://arxiv.org/pdf/1610.00211.pdf">https://arxiv.org/pdf/1610.00211.pdf</a></a><br />
</em><a href="https://link.springer.com/article/10.1007/s12652-017-0501-9">https://link.springer.com/article/10.1007/s12652-017-0501-9</a>  <br />
<em><a href="https://arxiv.org/pdf/1601.07754.pdf"><a href="https://arxiv.org/pdf/1601.07754.pdf">https://arxiv.org/pdf/1601.07754.pdf</a></a><br />
</em><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf</a><br />
<em><a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015ACMM_Scenes.pdf"><a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015ACMM_Scenes.pdf">http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015ACMM_Scenes.pdf</a></a><br />
</em><a href="https://arxiv.org/pdf/1511.02674.pdf">https://arxiv.org/pdf/1511.02674.pdf</a><br />
*<a href="http://videoanalysis.org/Prof._Dr._Rainer_Lienhart/Publications_files/MTAP2001.pdf">http://videoanalysis.org/Prof._Dr._Rainer_Lienhart/Publications_files/MTAP2001.pdf</a></li>
</ul>
<h1 id="todos"><a name="user-content-todos" href="#todos" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>Todos</h1>
<p><em>index of cutfiles and whether it&rsquo;s complete or not<br />
</em>get a decent definition of a program. commercials may exist within the same program. for e.g. news -&gt; commercial -&gt; short weather news -&gt; commercial should this count? (e.g. 2006-06-13_0000_US_00000433_V5_MB13_VHS14_H1_MS.txt3 ~3h15m)<br />
<em> file index (cutpoints, caption avaiable)<br />
</em>shortage of labelled samples: build a decent working classifier (that achieves 90+%) and let it generate samples. The supervising samples are considered labelled but extremely noisy so a noise-robust DNN model can learn from it.<br />
<em>noise (consistent, e.g 2006-06-13_v11)<br />
</em>just take this a single-frame image classification task? (but a stack of frames as input)</p></article></body></html>